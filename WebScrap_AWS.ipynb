{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRIZuhlvotvO",
        "outputId": "26f4a223-2389-4326-a2af-5e50c773b342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Company Name                 Company Website\n",
            "0              Adobe          https://www.adobe.com/\n",
            "1             Airbnb         https://www.airbnb.com/\n",
            "2     Alcatel-Lucent  https://www.al-enterprise.com/\n",
            "3                AOL            https://www.aol.com/\n",
            "4             Acquia         https://www.acquia.com/\n",
            "..               ...                             ...\n",
            "92        WeTransfer         https://wetransfer.com/\n",
            "93               WIX            https://www.wix.com/\n",
            "94            Xiaomi      https://www.mi.com/global/\n",
            "95              Yelp           https://www.yelp.com/\n",
            "96  Zynga and Zillow         https://www.zillow.com/\n",
            "\n",
            "[97 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#Scrapping 97 companies names and their website link that are using AWS\n",
        "\n",
        "from googlesearch import search\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def get_company_names(url):\n",
        "    html=requests.get(url)\n",
        "    if(html.status_code==200):\n",
        "        soup=BeautifulSoup(html.text,'html')\n",
        "        company_names=soup.find('em').text.split(', ')\n",
        "    else:\n",
        "        print(f\"Error\\nStatus Code : {html.status_code}\")\n",
        "    return company_names\n",
        "\n",
        "url='https://www.linkedin.com/pulse/biggest-aws-users-nikhil-suryawanshi/'\n",
        "company_names=get_company_names(url)\n",
        "\n",
        "def get_company_websites(company_names):\n",
        "    company_websites=[]\n",
        "    for company_name in company_names:\n",
        "        search_query=f\"{company_name} official website\"\n",
        "        for i in search(search_query):\n",
        "            company_websites.append(i)\n",
        "            break\n",
        "    return company_websites\n",
        "\n",
        "company_websites=get_company_websites(company_names)\n",
        "\n",
        "Details={'Company Name':company_names,'Company Website':company_websites}\n",
        "df=pd.DataFrame(Details,columns=['Company Name','Company Website'])\n",
        "df.to_csv(\"Amazon Web Services.csv\")\n",
        "print(df)"
      ]
    }
  ]
}